# -*- coding: utf-8 -*-
"""AI기초.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YBoKX_eo21dPEh6okNP-YbpRXM5d_x7M
"""

#구글 마운트하기
from google.colab import drive

drive.mount("/content/gdrive")

!gdown https://drive.google.com/uc?id=1a_dpOlLp7LjmOODaQ6UH5VnOhdE90Zet

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle competitions list

! kaggle competitions download -c titanic

! unzip /content/titanic.zip

#path 라는 변수에 타이타닉 경로 지정하기
path = '/content/train.csv'

#pandas 라이브러리 호출
import pandas as pd
titanic = pd.read_csv(path)

#데이터프레임.shape 은 데이터 프레임의 크기 (행과열)를 알려줌
print('데이터프레임의 크기: ', titanic.shape)

#데이터프렘임명.info() 는 데이터 프레임의 정보
print('데이터 프레임 정보 : ' , titanic.info())

#데이터프레임 내용 확인 head() 5행 먼저 보기
titanic.head()

#타이타닉 변수 보기
titanic

#데이터프레임의 속설 이름
print(titanic.columns)

#데이터프레임의 인덱스 정보
print(titanic.index)

#데이터프레임의 속성별 고유한 데이터의 개수
titanic.nunique()

#데이터프레임 속성별 범주 확인
titanic['Survived'].unique()

titanic['Sex'].unique()

titanic['Pclass'].unique()

titanic['Embarked'].unique()

#데이터 프레임의 통계정보
titanic.describe()

#결측 데이터 개수 보기
titanic.isna().sum()

#'AGA' 에 대한 결측 데이터 채우기 - 평균값

titanic['Age'] = titanic['Age'].fillna(titanic['Age'].mean())

#'Embarked' 에 대한 결측 데이터 채우기 -최빈값

titanic.groupby('Embarked')['PassengerId'].count()

titanic['Embarked']=titanic['Embarked'].fillna('S')
titanic.isna().sum()

#데이터 시각화
#성별 속성의 그룹별 생존자 확인

titanic.groupby(['Sex', 'Survived'])['Survived'].count()

import matplotlib.pyplot as plt
import seaborn as sns

sns.barplot(x='Sex', y='Survived', data = titanic)

#티켓등급에 따른 생존률 살펴보기

titanic.groupby(['Pclass', 'Survived'])['Survived'].count()

sns.barplot(x='Pclass', y='Survived', hue='Sex', data = titanic)

titanic.groupby(['Embarked', 'Survived'])['Survived'].count()

sns.barplot(x='Embarked', y='Survived', data = titanic)

!gdown https://drive.google.com/uc?id=1moW7rVsxIG2gHeTozsjisaS6ngC6fLh5
!gdown https://drive.google.com/uc?id=1dlmuMB7dC_hVEdLrg-AJMDUBMprBuEik

import cv2
from google.colab.patches import cv2_imshow
import numpy as np

#@title 이미지 경로 지정
path = '/content/KakaoTalk_20230503_095708214.jpg'

#@title 이미지 데이터 형식 출력
img = cv2.imread(path)

print(type(img))

img = cv2.imread(path)
cv2_imshow(img)

print(img.shape)
print(img.size)
print(img.dtype)

gray = cv2.imread(path, cv2.IMREAD_GRAYSCALE)

img

px = img[300,300]
print(px)
img[300, 300] = [255,255,255]
cv2_imshow(img)

subimg= img[140:250, 400:620]
cv2_imshow(img)

img2 = cv2.imread('/content/KakaoTalk_20230503_095713050.jpg')
img2 = cv2.resize(img2,(600,600),interpolation=cv2.INTER_LINEAR)

cv2_imshow(img2)

img1 =cv2.imread(path)
img = img1 +img2

cv2_imshow(img)

img = cv2.add(img1, img2)
cv2_imshow(img)
#양현준

!pip install konlpy

from konlpy.tag import Okt
okt=Okt()
sentence = '오월은 가정의달! 하지만 나는 바쁘다.'

print(okt.morphs(sentence))
print(okt.pos(sentence))
print(okt.nouns(sentence))

"""단어를 집합으로 변환하면 출력"""

sentence_2 = '서울 대전 대구 부산 광주 울산 인천 제주'
words = okt.morphs(sentence_2)
print(words)

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder

#문자를 정수형으로 변환

encoder = LabelEncoder()
encoder.fit(words)
result_1 = encoder.transform(words)
print(result_1)

result_2 = result_1.reshape(-1, 1)
print(result_2)

#@title 원한인코딩 적용
one_hot = OneHotEncoder()
one_hot.fit(result_2)
result_3 = one_hot.transform(result_2)
result_4 = result_3.toarray()
print(result_4)

#@title 빈도수 확인(단어가방)
from sklearn.feature_extraction.text import CountVectorizer
text_1 = '''
직합, 벡터는 텍스트 데이터에서 중요한 수학적 표현 도구 입니다.
특히 벡터는 단어의 빈도수를 표현할수 있습니다.
따라서 텍스트 데이터에서 벡터는 매우 중요합니다.
'''
corpus = [text_1]
vector =CountVectorizer()
print(vector.fit_transform(corpus).toarray())
print(sorted(vector.vocabulary_.items()))

from wordcloud import WordCloud

text_2 = '''
AI의 미래!, 하루가 다르게 발전하는 AI의 미래 기술은?
AI의 미래는 밝은 것일까?
좋던 실던 우리는 AI의 시대에 살고 있다.
'''
corpus = [text_2]
vector =CountVectorizer()
print(vector.fit_transform(corpus).toarray())
print(sorted(vector.vocabulary_))

wordcloud = WordCloud().generate(text_2)

!apt-get install fonts-nanum*

wordcloud = WordCloud(font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf',
    max_words = 50, background_color='white', max_font_size = 500, width =1000,
    height =1000)
wordcloud.generate(text_2)

plt.rcParams['figure.figsize'] = (10, 10)
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

